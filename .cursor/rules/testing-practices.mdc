---
description: Testing standards and practices.
globs: ["tests/**/*.py", "src/**/*.py"]
alwaysApply: false
---

# Testing Practices

## Test Organization

- Unit tests in `tests/unit/`.
- Integration tests in `tests/integration/`.
- End-to-end tests in `tests/e2e/`.
- Test files should start with `test_`.
- Test classes should start with `Test`.
- Test methods should start with `test_`.

## Test Standards

- Use descriptive test names that explain the scenario.
- Follow AAA pattern: Arrange, Act, Assert.
- Mock external dependencies (APIs, databases, file systems).
- Use fixtures for common test data and setup.
- Aim for >99% test coverage.
- Keep tests focused and independent.
- Write unit tests for all new functions.
- Add test coverage for every piece of expected behavior.

## Pytest Configuration

- Use pytest fixtures for setup and teardown.
- Set fixtures at the highest level possible to avoid redundant construction.
- Group related tests in classes when appropriate.
- Use parametrized tests for multiple scenarios.
- You may use other testing packages, like `unittest`. Just bear in mind that tests are run with the `pytest` CLI and configuration.

## Test Data Management

- Use factories for creating test data.
- Avoid hardcoded test data.
- Use realistic but minimal test datasets.
- Clean up test data after tests.
- Use ephemeral test databases when needed.
- Create tests that run on a live test database, but only optionally when that database is available.

## Mocking & Stubbing

- Mock external services and APIs.
- Use `unittest.mock` or `pytest-mock`.
- Mock file system operations for unit tests.
- Stub time-dependent operations.
- Mock network calls and external dependencies.

## Test Execution

- Run unit tests: `make unit`.
- Run integration tests: `make integration`.
- Run e2e tests: `make e2e`.
- Run all tests: `make full-test`.

## Test Quality

- Tests should be deterministic (no random data).
- Tests should be fast (unit tests < 1s each).
- Tests should be isolated (no shared state).
- Tests should be readable and maintainable.
- Tests should catch regressions effectively.

## Coverage & Reporting

- Aim for >99% code coverage.
- Focus on critical business logic.
- Generate coverage reports for analysis.
- Use coverage to identify untested code.

## Performance Testing

- Test image processing performance with large files.
- Monitor memory usage during tests.
- Test OCR accuracy with various image qualities.
- Benchmark critical data processing functions.
- Test with realistic dataset sizes.

## Integration Testing

- Test complete data extraction workflows.
- Test CLI commands end-to-end.
- Test error handling and recovery.
- Test with actual image samples.
- Test data validation and transformation.

## Test Maintenance

- Update tests when requirements change.
- Refactor tests to improve maintainability.
- Remove obsolete tests.
- Keep test data current with schema changes.
- Document complex test scenarios.
